<html>
  
  <head>
    
    <meta charset='utf-8'>
    <meta name='viewport' content='width=device-width, initial-scale=1, user-scalable=no'>
    <meta name='mobile-web-app-capable' content='yes'>
    <meta name='apple-mobile-web-app-capable' content='yes'>

    <title>WebXR Experiments</title>
    
  </head>
  
  <body>
  
    <header>
      <details open>
        <summary>WebXR Experiments</summary>
        <p>
          Hello There.
        </p>
      </details>
    </header>  



    <script type = "module">

      // Utility script to make the user consciously choose to enter the XR mode.
      import {WebXRButton} from './js/util/webxr-button.js';

      // 3D environment creation library.
      import {Scene} from './js/render/scenes/scene.js';

      // WebGL renderer.
      import {Renderer, createWebGLContext} from './js/render/core/renderer.js';
      
      // Skybox creator.
      import {SkyboxNode} from './js/render/nodes/skybox.js';
      
      // For inline VR mode.
      import {InlineViewerHelper} from './js/util/inline-viewer-helper.js';
      
      // Converting GLTF files for WebGL.
      import {Gltf2Node} from './js/render/nodes/gltf2.js';
      
      // Helper to check if certain features are to be applied (mainly used for Polyfill).
      import {QueryArgs} from './js/util/query-args.js';

      // If requested, use the polyfill to provide support for mobile devices
      // and devices which only support WebVR.
      import WebXRPolyfill from './js/third-party/webxr-polyfill/build/webxr-polyfill.module.js';
      if (QueryArgs.getBool('usePolyfill', true)) {
        let polyfill = new WebXRPolyfill();
      }

      // XR globals.
      let arButton = null;
      let vrButton = null;
      let xrImmersiveRefSpace = null;
      let inlineViewerHelper = null;

      // WebGL scene globals.
      // GL, Renderer, Scene
      let gl = null;
      let renderer = null;
      let scene = new Scene();
      
      // Solar System Object Node
      let solarSystem = new Gltf2Node({url: 'media/gltf/space/space.gltf'});      
      // The solar system is big (citation needed). Scale it down so that users
      // can move around the planets more easily.      
      solarSystem.scale = [0.1, 0.1, 0.1];
      scene.addNode(solarSystem);
      
      // Still adding a skybox, but only for the benefit of the inline view.
      let skybox = new SkyboxNode({url: 'media/textures/milky-way-4k.png'});
      scene.addNode(skybox);

      function initXR() {

        vrButton = new WebXRButton({
          onRequestSession: onRequestSessionVR,
          onEndSession: onEndVRSession
        });
        document.querySelector('header').appendChild(vrButton.domElement);


        arButton = new WebXRButton({
          onRequestSession: onRequestSessionAR,
          onEndSession: onEndARSession,
          textEnterXRTitle: "START AR",
          textXRNotFoundTitle: "AR NOT FOUND",
          textExitXRTitle: "EXIT AR",
        });
        document.querySelector('header').appendChild(arButton.domElement);

        // Is WebXR available on this UA?
        if (navigator.xr) {
          // Checks to ensure that 'immersive-ar' mode is available, and only
          // enables the button if so.
          navigator.xr.isSessionSupported('immersive-ar').then((supported) => {
            arButton.enabled = supported;
          });

          navigator.xr.isSessionSupported('immersive-vr').then((supported) => {
            vrButton.enabled = supported;
          });

          navigator.xr.requestSession('inline').then(onARSessionStarted);
        }
      }


      // Called when the user selects a device to present to. In response we
      // will request an exclusive session from that device.
      function onRequestSessionAR() {
        // Requests an 'immersive-ar' session, which ensures that the users
        // environment will be visible either via video passthrough or a
        // transparent display. This may be presented either in a headset or
        // fullscreen on a mobile device.
        return navigator.xr.requestSession('immersive-ar')
            .then((session) => {
              arButton.setSession(session);
              session.isImmersive = true;
              onARSessionStarted(session);
            });      
      }

      //

      function onRequestSessionVR() {
        return navigator.xr.requestSession('immersive-vr').then(onVRSessionStarted);
      }

      //

      function initGL() {
        if (gl)
          return;

        gl = createWebGLContext({
          xrCompatible: true
        });
        document.body.appendChild(gl.canvas);

        function onResize() {
          gl.canvas.width = gl.canvas.clientWidth * window.devicePixelRatio;
          gl.canvas.height = gl.canvas.clientHeight * window.devicePixelRatio;
        }
        window.addEventListener('resize', onResize);
        onResize();

        renderer = new Renderer(gl);

        scene.setRenderer(renderer);
      }
        

      //

      function onARSessionStarted(session) {
        session.addEventListener('end', onARSessionEnded);

        if (session.isImmersive) {
          // When in 'immersive-ar' mode don't draw an opaque background because
          // we want the real world to show through.
          skybox.visible = false;
        }

        initGL();

        session.updateRenderState({ baseLayer: new XRWebGLLayer(session, gl) });

        let refSpaceType = session.isImmersive ? 'local' : 'viewer';
        session.requestReferenceSpace(refSpaceType).then((refSpace) => {
          if (session.isImmersive) {
            xrImmersiveRefSpace = refSpace;
          } else {
            inlineViewerHelper = new InlineViewerHelper(gl.canvas, refSpace);
          }
          session.requestAnimationFrame(onARFrame);
        });
      }

      //

      function onEndARSession(session) {
        session.end();
      }


      //


      function onARSessionEnded(event) {
        if (event.session.isImmersive) {
          arButton.setSession(null);
          // Turn the background back on when we go back to the inlive view.
          skybox.visible = true;
        }
      }

      // Called every time a XRSession requests that a new frame be drawn.
      function onARFrame(t, frame) {
        let session = frame.session;
        let refSpace = session.isImmersive ?
                         xrImmersiveRefSpace :
                         inlineViewerHelper.referenceSpace;
        let pose = frame.getViewerPose(refSpace);

        scene.startFrame();

        session.requestAnimationFrame(onARFrame);

        scene.drawXRFrame(frame, pose);

        scene.endFrame();
      }


      //

      function onVRSessionStarted(session) {
        // This informs the 'Enter XR' button that the session has started and
        // that it should display 'Exit XR' instead.
        vrButton.setSession(session);

        // Listen for the sessions 'end' event so we can respond if the user
        // or UA ends the session for any reason.
        session.addEventListener('end', onVRSessionEnded);

        // Create a WebGL context to render with, initialized to be compatible
        // with the XRDisplay we're presenting to.
        gl = createWebGLContext({
          xrCompatible: true
        });

        // Create a renderer with that GL context (this is just for the samples
        // framework and has nothing to do with WebXR specifically.)
        renderer = new Renderer(gl);

        // Set the scene's renderer, which creates the necessary GPU resources.
        scene.setRenderer(renderer);

        // Use the new WebGL context to create a XRWebGLLayer and set it as the
        // sessions baseLayer. This allows any content rendered to the layer to
        // be displayed on the XRDevice.
        session.updateRenderState({ baseLayer: new XRWebGLLayer(session, gl) });

        // Get a frame of reference, which is required for querying poses. In
        // this case an 'local' frame of reference means that all poses will
        // be relative to the location where the XRDevice was first detected.
        session.requestReferenceSpace('local').then((refSpace) => {
          xrRefSpace = refSpace;

          // Inform the session that we're ready to begin drawing.
          session.requestAnimationFrame(onVRFrame);
        });
      }


      // Called when the user clicks the 'Exit XR' button. In response we end
      // the session.
      function onEndVRSession(session) {
        session.end();
      }

      // Called either when the user has explicitly ended the session (like in
      // onEndSession()) or when the UA has ended the session for any reason.
      // At this point the session object is no longer usable and should be
      // discarded.
      function onVRSessionEnded(event) {
        vrButton.setSession(null);

        // In this simple case discard the WebGL context too, since we're not
        // rendering anything else to the screen with it.
        renderer = null;
      }

      //
      // Called every time the XRSession requests that a new frame be drawn.
      function onVRFrame(t, frame) {
        let session = frame.session;

        // Per-frame scene setup. Nothing WebXR specific here.
        scene.startFrame();

        // Inform the session that we're ready for the next frame.
        session.requestAnimationFrame(onVRFrame);

        // Get the XRDevice pose relative to the Frame of Reference we created
        // earlier.
        let pose = frame.getViewerPose(xrRefSpace);

        // Getting the pose may fail if, for example, tracking is lost. So we
        // have to check to make sure that we got a valid pose before attempting
        // to render with it. If not in this case we'll just leave the
        // framebuffer cleared, so tracking loss means the scene will simply
        // disappear.
        if (pose) {
          let glLayer = session.renderState.baseLayer;

          // If we do have a valid pose, bind the WebGL layer's framebuffer,
          // which is where any content to be displayed on the XRDevice must be
          // rendered.
          gl.bindFramebuffer(gl.FRAMEBUFFER, glLayer.framebuffer);

          // Clear the framebuffer
          gl.clear(gl.COLOR_BUFFER_BIT | gl.DEPTH_BUFFER_BIT);

          // Loop through each of the views reported by the frame and draw them
          // into the corresponding viewport.
          for (let view of pose.views) {
            let viewport = glLayer.getViewport(view);
            gl.viewport(viewport.x, viewport.y,
                        viewport.width, viewport.height);

            // Draw this view of the scene. What happens in this function really
            // isn't all that important. What is important is that it renders
            // into the XRWebGLLayer's framebuffer, using the viewport into that
            // framebuffer reported by the current view, and using the
            // projection matrix and view transform from the current view.
            // We bound the framebuffer and viewport up above, and are passing
            // in the appropriate matrices here to be used when rendering.
            scene.draw(view.projectionMatrix, view.transform);
          }
        } else {
          // There's several options for handling cases where no pose is given.
          // The simplest, which these samples opt for, is to simply not draw
          // anything. That way the device will continue to show the last frame
          // drawn, possibly even with reprojection. Alternately you could
          // re-draw the scene again with the last known good pose (which is now
          // likely to be wrong), clear to black, or draw a head-locked message
          // for the user indicating that they should try to get back to an area
          // with better tracking. In all cases it's possible that the device
          // may override what is drawn here to show the user it's own error
          // message, so it should not be anything critical to the application's
          // use.
        }

        // Per-frame scene teardown. Nothing WebXR specific here.
        scene.endFrame();
      }

      initXR();

    </script>

  </body>
  
</html>
